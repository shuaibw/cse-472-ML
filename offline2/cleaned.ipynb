{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, RobustScaler, KBinsDiscretizer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "np.random.seed(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1_path = 'data/telco/WA_Fn-UseC_-Telco-Customer-Churn.csv'\n",
    "data2_path = 'data/adult/adult.data'\n",
    "data2_extra = 'data/adult/adult.test'\n",
    "data3_path = 'data/creditcardfraud/creditcard.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset1(data1_path):\n",
    "    df = pd.read_csv(data1_path)\n",
    "    df = df.drop('customerID', axis=1)\n",
    "    X, y = df.drop('Churn', axis=1), df['Churn']\n",
    "    X['TotalCharges'] = pd.to_numeric(X['TotalCharges'], errors='coerce')\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    categorical_cols = [cname for cname in X_train.columns if X_train[cname].dtype == \"object\"]\n",
    "    numerical_cols = [cname for cname in X_train.columns if X_train[cname].dtype in ['int64', 'float64']]\n",
    "    \n",
    "    my_cols = categorical_cols + numerical_cols\n",
    "    X_train = X_train[my_cols].copy()\n",
    "    X_test = X_test[my_cols].copy()\n",
    "    \n",
    "    numerical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='mean')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "    ])\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numerical_transformer, numerical_cols),\n",
    "            ('cat', categorical_transformer, categorical_cols)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
    "    X_train_preprocessed = pd.DataFrame(X_train_preprocessed)\n",
    "    bias_train = pd.DataFrame(np.ones((X_train_preprocessed.shape[0],1)))\n",
    "    X_train_preprocessed = pd.concat([bias_train, X_train_preprocessed], axis=1)\n",
    "    \n",
    "    X_test_preprocessed = preprocessor.transform(X_test)\n",
    "    X_test_preprocessed = pd.DataFrame(X_test_preprocessed)\n",
    "    bias_test = pd.DataFrame(np.ones((X_test_preprocessed.shape[0],1)))\n",
    "    X_test_preprocessed = pd.concat([bias_test, X_test_preprocessed], axis=1)\n",
    "    \n",
    "    y_train_preprocessed = y_train.replace({'No': 0, 'Yes': 1})\n",
    "    y_test_preprocessed = y_test.replace({'No': 0, 'Yes': 1})\n",
    "    return X_train_preprocessed, y_train_preprocessed, X_test_preprocessed, y_test_preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset2(data2_train_path, data2_test_path):\n",
    "    data2_headers = ['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'income']\n",
    "    df2_train = pd.read_csv(data2_train_path, names=data2_headers, index_col=False)\n",
    "    df2_test = pd.read_csv(data2_test_path, names=data2_headers, index_col=False, skiprows=1)\n",
    "    len_train = len(df2_train)\n",
    "    df2_all = pd.concat([df2_train, df2_test])\n",
    "    df2_all['income'] = df2_all['income'].replace({' <=50K.': ' <=50K', ' >50K.': ' >50K'})\n",
    "    \n",
    "    # df2_all = df2_all.drop('native-country', axis=1)\n",
    "    \n",
    "    # cluster the native-country column\n",
    "    # richest = [' United-States',' Canada' ,' England', ' France', ' Japan' ,' Germany', ' Greece', ' Holand-Netherlands', ' Hungary', ' Ireland', ' Italy', ' Poland', ' Portugal', ' Scotland']\n",
    "    # medium = [' Cuba', ' China' ,' Taiwan' ,' Dominican-Republic', ' El-Salvador', ' Iran', ' Outlying-US(Guam-USVI-etc)', ' Guatemala', ' Haiti', ' Honduras', ' Jamaica', ' Mexico', ' Nicaragua', ' Puerto-Rico', ' Trinadad&Tobago', ' Ecuador', ' Peru', ' Columbia']\n",
    "    # poorest = [' Cambodia', ' Hong', ' India', ' Laos', ' Philippines', ' Thailand', ' Vietnam']\n",
    "    # others = [' ?', ' South', ' Yugoslavia']\n",
    "    # df2_all['native-country'] = df2_all['native-country'].replace(richest, 'Richest')\n",
    "    # df2_all['native-country'] = df2_all['native-country'].replace(medium, 'Medium')\n",
    "    # df2_all['native-country'] = df2_all['native-country'].replace(poorest, 'Poorest')\n",
    "    # df2_all['native-country'] = df2_all['native-country'].replace(others, 'Other')\n",
    "    \n",
    "    # cleanup the age column\n",
    "    df2_all['age'] = pd.to_numeric(df2_all['age'], errors='coerce')\n",
    "    # drop the row where age is NaN\n",
    "    df2_all = df2_all.dropna(subset=['age'])\n",
    "    \n",
    "    df2_train = df2_all.iloc[:len_train]\n",
    "    df2_test = df2_all.iloc[len_train:]\n",
    "    \n",
    "    X_train, y_train = df2_train.drop('income', axis=1), df2_train['income']\n",
    "    X_test, y_test = df2_test.drop('income', axis=1), df2_test['income']\n",
    "\n",
    "    categorical_cols = [cname for cname in X_train.columns if X_train[cname].dtype == \"object\"]\n",
    "    numerical_cols = [cname for cname in X_train.columns if X_train[cname].dtype in ['int64', 'float64']]\n",
    "    \n",
    "    numerical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "    ])\n",
    "\n",
    "    preprocessor = ColumnTransformer([\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ])\n",
    "    \n",
    "    X_train_processed= preprocessor.fit_transform(X_train)\n",
    "    X_train_processed = pd.DataFrame(X_train_processed)\n",
    "    bias_train = pd.DataFrame(np.ones((X_train_processed.shape[0],1)), columns=['bias'])\n",
    "    X_train_processed = pd.concat([bias_train, X_train_processed], axis=1)\n",
    "    \n",
    "    X_test_processed = preprocessor.transform(X_test)\n",
    "    X_test_processed = pd.DataFrame(X_test_processed)\n",
    "    bias_test = pd.DataFrame(np.ones((X_test_processed.shape[0],1)), columns=['bias'])\n",
    "    X_test_processed = pd.concat([bias_test, X_test_processed], axis=1)\n",
    "    \n",
    "    y_train_processed = y_train.replace({' <=50K': 0, ' >50K': 1})\n",
    "    y_test_processed = y_test.replace({' <=50K': 0, ' >50K': 1})\n",
    "    \n",
    "    return X_train_processed, y_train_processed, X_test_processed, y_test_processed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_score_outlier_removal(df, column, threshold=3):\n",
    "    df = df.copy()\n",
    "    mean = df[column].mean()\n",
    "    std = df[column].std()\n",
    "    df['z_score'] = (df[column] - mean)/std\n",
    "    df = df[df['z_score'].abs() < threshold]\n",
    "    df = df.drop('z_score', axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset3(data3_path):\n",
    "    df = pd.read_csv(data3_path)\n",
    "    df = df.sample(frac=1, random_state=42)\n",
    "    df = df.drop('Time', axis=1)\n",
    "    fraud_df = df.loc[df['Class'] == 1]\n",
    "    non_fraud_df = df.loc[df['Class'] == 0][:1000]\n",
    "    balanced_df = pd.concat([fraud_df, non_fraud_df])\n",
    "    balanced_df = balanced_df.sample(frac=1, random_state=42)\n",
    "    X, y = balanced_df.drop('Class', axis=1), balanced_df['Class']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    scaler = Pipeline(steps=[\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "    \n",
    "    preprocessor = ColumnTransformer([\n",
    "        ('num', scaler, ['Amount']),\n",
    "        # nothing on other columns\n",
    "        ('other', 'passthrough', X_train.columns.difference(['Amount']))\n",
    "    ])\n",
    "    \n",
    "    X_train = preprocessor.fit_transform(X_train)\n",
    "    X_train = pd.DataFrame(X_train)\n",
    "    bias_train = pd.DataFrame(np.ones((X_train.shape[0],1)), columns=['bias'])\n",
    "    X_train = pd.concat([bias_train, X_train], axis=1)\n",
    "    \n",
    "    X_test = preprocessor.transform(X_test)\n",
    "    X_test = pd.DataFrame(X_test)\n",
    "    bias_test = pd.DataFrame(np.ones((X_test.shape[0],1)), columns=['bias'])\n",
    "    X_test = pd.concat([bias_test, X_test], axis=1)\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretize_numerical_columns(X, numerical_cols, n_bins=5):\n",
    "    discretizer = KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy='uniform')\n",
    "    # Fit and transform the data\n",
    "    X_binned = discretizer.fit_transform(X[numerical_cols])\n",
    "    # Convert to DataFrame with column names\n",
    "    X_binned = pd.DataFrame(X_binned, columns=numerical_cols)\n",
    "    # Return the binned numerical data\n",
    "    return X_binned\n",
    "\n",
    "def entropy(target_col):\n",
    "    elements, counts = np.unique(target_col, return_counts=True)\n",
    "    entropy = np.sum([(-counts[i] / np.sum(counts)) * np.log2(counts[i] / np.sum(counts)) for i in range(len(elements))])\n",
    "    return entropy\n",
    "\n",
    "def InfoGain(data, split_attribute_name, target_name=\"class\"):\n",
    "    total_entropy = entropy(data[target_name])\n",
    "    vals, counts = np.unique(data[split_attribute_name], return_counts=True)\n",
    "    weighted_entropy = np.sum([(counts[i] / np.sum(counts)) * entropy(data[data[split_attribute_name] == vals[i]][target_name]) for i in range(len(vals))])\n",
    "    information_gain = total_entropy - weighted_entropy\n",
    "    return information_gain\n",
    "\n",
    "def feature_selection(X, y, num_features=10):\n",
    "    categorical_cols = []\n",
    "    numerical_cols = []\n",
    "    for col in X.columns[1:]:  # Note: Changed 'range(len(X.columns[1:]))' to 'X.columns[1:]'\n",
    "        if X[col].nunique() == 2:\n",
    "            categorical_cols.append(col)\n",
    "        else:\n",
    "            numerical_cols.append(col)\n",
    "            \n",
    "    print(f'Categorical columns: {categorical_cols}')\n",
    "    print(f'Numerical columns: {numerical_cols}')\n",
    "\n",
    "    # Combine X and y for information gain calculation\n",
    "    data = X.copy()\n",
    "    target = y.copy()\n",
    "    data[numerical_cols] = discretize_numerical_columns(data, numerical_cols)\n",
    "    target = pd.DataFrame(target)\n",
    "    target.columns = ['class']\n",
    "    data.reset_index(drop=True, inplace=True)\n",
    "    target.reset_index(drop=True, inplace=True)\n",
    "    data = pd.concat([data, target], axis=1)\n",
    "\n",
    "    # Calculate information gain for each categorical column\n",
    "    info_gains = {col: InfoGain(data, col, 'class') for col in categorical_cols+numerical_cols}\n",
    "\n",
    "    # Sort columns based on information gain and select top num_features\n",
    "    top_categorical_features = sorted(info_gains, key=info_gains.get, reverse=True)[:num_features]\n",
    "\n",
    "    return top_categorical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def predict(X, theta):\n",
    "    p = np.round(sigmoid(np.dot(X, theta)))\n",
    "    return p\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, lambda_=0, error_thresh=0.5):\n",
    "        self.lambda_ = lambda_\n",
    "        self.theta = None\n",
    "        self.error_thresh = error_thresh\n",
    "    \n",
    "    def lrCostFunction(self, X, y):\n",
    "        m = y.size\n",
    "        J = 0\n",
    "        grad = np.zeros(self.theta.shape)\n",
    "        \n",
    "        h = sigmoid(np.dot(X, self.theta))\n",
    "        h[h==0]=1e-6\n",
    "        h[h==1]=1-1e-6\n",
    "        J = (1/m) * np.sum((-y * np.log(h)) - ((1-y) * np.log(1-h))) + ((self.lambda_/(2*m)) * np.sum(np.square(self.theta[1:])))\n",
    "        grad = (1/m) * np.dot(X.T, (h-y))\n",
    "        grad[1:] = grad[1:] + ((self.lambda_/m) * self.theta[1:])\n",
    "        \n",
    "        return J, grad\n",
    "    \n",
    "    def gradientDescent(self, X, y, alpha, num_iters):\n",
    "        m = y.shape[0]\n",
    "        self.theta = np.random.random_sample((X.shape[1], )) - 0.5\n",
    "        J_history = []\n",
    "        \n",
    "        for i in range(num_iters):\n",
    "            J, grad = self.lrCostFunction(X, y)\n",
    "            self.theta = self.theta - (alpha * grad)\n",
    "            J_history.append(J)\n",
    "            if J < self.error_thresh:\n",
    "                break\n",
    "        \n",
    "        return self.theta, J_history\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.round(sigmoid(np.dot(X, self.theta)))\n",
    "\n",
    "def AdaBoostClassifier(X, y, n_estimators=50, learner=LogisticRegression):\n",
    "    N = X.shape[0]\n",
    "    n_samples = int(N * 0.55)\n",
    "    W = np.full(N, (1 / N))\n",
    "    h = np.zeros(shape=(n_estimators, X.shape[1]))\n",
    "    z = np.zeros(n_estimators)\n",
    "\n",
    "    for k in tqdm(range(n_estimators)):\n",
    "        # Train a weak learner\n",
    "        lr_clf = learner()\n",
    "        # pick X_train, y_train with replacement according to W\n",
    "        indices = np.random.choice(N, n_samples, p=W, replace=False)\n",
    "        X_train = X.iloc[indices]\n",
    "        y_train = y.iloc[indices]\n",
    "        \n",
    "        h[k], _ = lr_clf.gradientDescent(X_train, y_train, 0.1, 10000)\n",
    "        # plt.plot(_)\n",
    "        # plt.show()        \n",
    "        # Calculate error\n",
    "        error = 0\n",
    "        # for j in range(n_samples):\n",
    "        #     if predict(X[j], h[k]) != y[j]:\n",
    "        #         error += W[j]\n",
    "        # find indices of misclassified samples\n",
    "        misclassified_indices = np.where(predict(X, h[k]) != y)[0]\n",
    "        error = np.sum(W[misclassified_indices])\n",
    "        if error > 0.5:\n",
    "            continue\n",
    "        error = max(error, 1e-5)\n",
    "        # for j in range(n_samples):\n",
    "        #     if predict(X[j], h[k]) == y[j]:\n",
    "        #         W[j] *= error / (1 - error)\n",
    "        classified_indices = np.where(predict(X, h[k]) == y)[0]\n",
    "        W[classified_indices] *= error / (1 - error)\n",
    "        # Normalize sample weights\n",
    "        W /= np.sum(W)\n",
    "        # Save the current estimator and its weight\n",
    "        z[k] = np.log((1 - error) / error)\n",
    "    z = z / np.sum(z)\n",
    "    \n",
    "    def predict_final(X):\n",
    "        pred = np.sign(sum([z[i] * (2*sigmoid(np.dot(X, h[i]))-1) for i in range(n_estimators)]))\n",
    "        pred[pred == -1] = 0\n",
    "        return pred\n",
    "    return predict_final\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_adaboost_analysis(X_train, y_train, X_test, y_test, title='Bl'):\n",
    "    # Vary learners from 1 to 50 in steps of 5, and plot the accuracy, precision, recall and F1 scores on the test set\n",
    "    learners = np.arange(0, 51, 5)\n",
    "    learners[0] = 1\n",
    "    accuracy = []\n",
    "    precision = []\n",
    "    recall = []\n",
    "    f1 = []\n",
    "    for n in learners:\n",
    "        ada_clf = AdaBoostClassifier(X_train, y_train, n)\n",
    "        y_pred = ada_clf(X_test)\n",
    "        y_pred[y_pred==-1] = 0\n",
    "        accuracy.append(accuracy_score(y_test, y_pred))\n",
    "        precision.append(precision_score(y_test, y_pred))\n",
    "        recall.append(recall_score(y_test, y_pred))\n",
    "        f1.append(f1_score(y_test, y_pred))\n",
    "\n",
    "    plt.plot(learners, accuracy, label='Accuracy')\n",
    "    plt.plot(learners, precision, label='Precision')\n",
    "    plt.plot(learners, recall, label='Recall')\n",
    "    plt.plot(learners, f1, label='F1')\n",
    "    plt.xlabel('Number of learners')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.savefig(f'{title}.png', dpi=400, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    for num_learners,a,p,r,f in zip(learners, accuracy, precision, recall, f1):\n",
    "        print(f'Learners: {num_learners} --> Accuracy: {a:.2f}, Precision: {p:.2f}, Recall: {r:.2f}, F1: {f:.2f}')\n",
    "        \n",
    "def only_logistic(X_train, y_train, X_test, y_test, learning_rate=1, num_iters=5000, error_thresh=0.0000001):\n",
    "    lr_clf = LogisticRegression(error_thresh=error_thresh)\n",
    "    theta, _ = lr_clf.gradientDescent(X_train, y_train, learning_rate, num_iters)\n",
    "    print('Test set metrics for Logistic Regression:')\n",
    "    y_pred = lr_clf.predict(X_test)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "    specificity = tn / (tn+fp)\n",
    "    fdr = fp / (fp+tp)\n",
    "    print(f'Accuracy: {accuracy_score(y_test, y_pred):.2f}, Recall: {recall_score(y_test, y_pred):.2f}, Specificity: {specificity:.2f}, Precision: {precision_score(y_test, y_pred):.2f}, FDR: {fdr:.2f}, F1: {f1_score(y_test, y_pred):.2f}')\n",
    "    \n",
    "    print('Train set metrics for Logistic Regression:')\n",
    "    y_pred = lr_clf.predict(X_train)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_train, y_pred).ravel()\n",
    "    specificity = tn / (tn+fp)\n",
    "    fdr = fp / (fp+tp)\n",
    "    print(f'Accuracy: {accuracy_score(y_train, y_pred):.2f}, Recall: {recall_score(y_train, y_pred):.2f}, Specificity: {specificity:.2f}, Precision: {precision_score(y_train, y_pred):.2f}, FDR: {fdr:.2f}, F1: {f1_score(y_train, y_pred):.2f}')\n",
    "    \n",
    "    return _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = preprocess_dataset3(data3_path)\n",
    "run_adaboost_analysis(X_train, y_train, X_test, y_test, 'Credit Card Fraud')\n",
    "print('--------------------------------------')\n",
    "J_hist = only_logistic(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = preprocess_dataset2(data2_path, data2_extra)\n",
    "\n",
    "selected = feature_selection(X_train, y_train, 50)\n",
    "X_train = X_train[selected]\n",
    "X_test = X_test[selected]\n",
    "\n",
    "run_adaboost_analysis(X_train, y_train, X_test, y_test, 'Adult Census Income')\n",
    "print('-----------------------------------------')\n",
    "J_hist=only_logistic(X_train, y_train, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = preprocess_dataset1(data1_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_adaboost_analysis(X_train, y_train, X_test, y_test, 'Telco Customer Churn')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('-----------------------------------------')\n",
    "J_hist = only_logistic(X_train, y_train, X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
