{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30626,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import f1_score, confusion_matrix\nfrom tqdm import tqdm\nimport gc\nimport torchvision.datasets as ds\nimport torchvision.transforms as transforms\nimport pickle\n\nclass Layer:\n    def __init__(self):\n        pass\n\n    def forward(self, A):\n        pass\n\n    def backward(self, dZ):\n        pass\n\nclass DenseLayer(Layer):\n    def __init__(self, input_size, output_size, activation='relu'):\n        self.W = np.random.randn(output_size, input_size) * np.sqrt(2/input_size) # He initialization\n        self.b = np.zeros((output_size, 1))\n        self.activation_function = ActivationLayer(activation)\n\n    def forward(self, A):\n        Z = self.W.dot(A) + self.b\n        assert Z.shape == (self.W.shape[0], A.shape[1])\n        return Z\n\n    def backward(self, dZ, A_prev):\n        m = A_prev.shape[1]\n        dW = 1./m * np.dot(dZ, A_prev.T)\n        db = 1./m * np.sum(dZ, axis=1, keepdims=True)\n        dA_prev = np.dot(self.W.T, dZ)\n        \n        assert dA_prev.shape == A_prev.shape\n        assert dW.shape == self.W.shape\n        assert db.shape == self.b.shape\n        \n        return dA_prev, dW, db\n\nclass DropoutLayer(Layer):\n    def __init__(self, rate):\n        self.rate = rate\n        self.mask = None\n\n    def forward(self, A):\n        self.mask = np.random.rand(*A.shape) > self.rate\n        A = A * self.mask\n        A /= (1 - self.rate)\n        return A\n\n    def backward(self, dA):\n        dA = dA * self.mask\n        dA /= (1 - self.rate)\n        return dA\n\nclass ActivationLayer(Layer):\n    def __init__(self, activation):\n        self.activation = activation\n        self.activation_cache = None\n\n    def forward(self, Z):\n        self.activation_cache = Z\n        if self.activation == \"relu\":\n            A = np.maximum(0, Z)\n            assert A.shape == Z.shape\n            return A\n        elif self.activation == \"sigmoid\":\n            A = 1 / (1 + np.exp(-Z))\n            assert A.shape == Z.shape\n            return A\n        elif self.activation == \"softmax\":\n            Z = np.clip(Z, -20, 20)\n            e_x = np.exp(Z)\n            A = e_x / np.sum(e_x, axis=0)\n            assert A.shape == Z.shape\n            return A + 1e-8\n\n    def backward(self, dA):\n        Z = self.activation_cache\n        if self.activation == \"relu\":\n            dZ = np.array(dA, copy=True)\n            dZ[Z <= 0] = 0\n            assert dZ.shape == Z.shape\n            return dZ\n        elif self.activation == \"sigmoid\":\n            s = 1 / (1 + np.exp(-Z))\n            dZ = dA * s * (1 - s)\n            assert dZ.shape == Z.shape\n            return dZ\n        elif self.activation == \"softmax\":\n            # For this case, dA = Y_true\n            Z = np.clip(Z, -20, 20)\n            e_x = np.exp(Z)\n            s = e_x / np.sum(e_x, axis=0)\n            dZ = s - dA\n            assert dZ.shape == Z.shape\n            return dZ\nclass Network:\n    def __init__(self):\n        self.layers = []\n        self.cache = []\n        self.costs = []\n        self.train_acc = []\n        self.test_acc = []\n        self.test_costs = []\n        self.test_f1_scores = []\n        self.linestyle = '-'\n        # For adam optimizer\n        self.beta1 = 0.9\n        self.beta2 = 0.999\n        self.epsilon = 1e-8\n        self.t = 0\n        self.optimizer = None\n\n    def add(self, layer):\n        self.layers.append(layer)\n\n    def forward(self, X, training=True):\n        A = X\n        self.cache = [(None, X)]\n        for layer in self.layers:\n            if isinstance(layer, DropoutLayer) and training:\n                A = layer.forward(A)\n            elif isinstance(layer, DenseLayer):\n                Z = layer.forward(A)\n                A = layer.activation_function.forward(Z)\n                self.cache.append((Z, A))\n        assert A.shape == (26, X.shape[1])\n        return A + 1e-8\n\n    def backward(self, Y, AL):\n        gradients = {}\n        Y = Y.reshape(AL.shape)\n        L = len(self.layers)\n        dA_prev = Y  # Assuming last layer uses softmax\n        self.cache.pop()\n        for i in reversed(range(L)):\n            layer = self.layers[i]\n            if isinstance(layer, DropoutLayer):\n                dA_prev = layer.backward(dA_prev)\n            elif isinstance(layer, DenseLayer):\n                self.t += 1\n                Z, A_prev = self.cache.pop()\n                dZ = layer.activation_function.backward(dA_prev)\n                dA_prev, dW, db = layer.backward(dZ, A_prev)\n                if self.optimizer == \"adam\":\n                    gradients[\"vdW\" + str(i+1)] = self.beta1 * gradients.get(\"vdW\" + str(i+1), np.zeros_like(dW)) + (1 - self.beta1) * dW\n                    gradients[\"vdb\" + str(i+1)] = self.beta1 * gradients.get(\"vdb\" + str(i+1), np.zeros_like(db)) + (1 - self.beta1) * db\n                    gradients[\"sdW\" + str(i+1)] = self.beta2 * gradients.get(\"sdW\" + str(i+1), np.zeros_like(dW)) + (1 - self.beta2) * dW**2\n                    gradients[\"sdb\" + str(i+1)] = self.beta2 * gradients.get(\"sdb\" + str(i+1), np.zeros_like(db)) + (1 - self.beta2) * db**2\n                    vdW_corrected = gradients[\"vdW\" + str(i+1)] / (1 - self.beta1**self.t)\n                    vdb_corrected = gradients[\"vdb\" + str(i+1)] / (1 - self.beta1**self.t)\n                    sdW_corrected = gradients[\"sdW\" + str(i+1)] / (1 - self.beta2**self.t)\n                    sdb_corrected = gradients[\"sdb\" + str(i+1)] / (1 - self.beta2**self.t)\n                    dW = vdW_corrected / (np.sqrt(sdW_corrected) + self.epsilon)\n                    db = vdb_corrected / (np.sqrt(sdb_corrected) + self.epsilon)\n                gradients[\"dW\" + str(i+1)] = dW\n                gradients[\"db\" + str(i+1)] = db\n        return gradients\n\n    def update_parameters(self, gradients):\n        for i, layer in enumerate(self.layers):\n            if isinstance(layer, DenseLayer):\n                layer.W -= self.learning_rate * gradients[\"dW\" + str(i+1)]\n                layer.b -= self.learning_rate * gradients[\"db\" + str(i+1)]\n\n    def train(self, X_train, y_train, epochs, learning_rate=0.1, batch_size=64, valid_split=0.2, optimizer=None, decay_rate=0, linestyle='-'):\n        self.linestyle = linestyle\n        self.learning_rate = learning_rate\n        fixed_lr = learning_rate\n        self.optimizer = optimizer\n        X_train, y_train, X_valid, y_valid = self.train_test_split(X_train, y_train, valid_split=valid_split)\n        num_examples = X_train.shape[1]\n        num_batches = num_examples // batch_size\n        for epoch in range(epochs):\n            train_cost = 0\n            self.learning_rate = fixed_lr * np.exp(-decay_rate*epoch)\n            for j in tqdm(range(0, num_examples, batch_size)):\n                start = j\n                end = min(j+batch_size, num_examples)\n                X_batch = X_train[:, start:end]\n                y_batch = y_train[:, start:end]\n                output = self.forward(X_batch)\n                cost = self.compute_cost(output, y_batch)\n                train_cost += cost\n                gradients = self.backward(y_batch, output)\n                self.update_parameters(gradients)\n\n            # Compute cost and accuracy\n            test_cost = self.compute_cost(self.forward(X_valid), y_valid)\n            train_cost /= num_batches\n            _, train_acc = self.predict(X_train, y_train)\n            test_preds, test_acc = self.predict(X_valid, y_valid)\n            test_f1 = f1_score(np.argmax(y_valid, axis=0), test_preds, average='macro')\n            self.costs.append(train_cost)\n            self.test_costs.append(test_cost)\n            self.train_acc.append(train_acc)\n            self.test_acc.append(test_acc)\n            self.test_f1_scores.append(test_f1)\n            \n            print(f'Epoch: {epoch+1}/{epochs}, train_cost: {train_cost:.5f}, test_cost: {test_cost:.5f}, train_acc: {train_acc:.5f}, test_acc: {test_acc:.5f}')\n            \n    def predict(self, X, y):\n        m = X.shape[1]   # X = (768, m)\n        # y = (26, m)\n        y_hat = self.forward(X, training=False)\n        p = np.argmax(y_hat, axis=0)\n        y = np.argmax(y, axis=0)\n        correct = np.sum(p == y)\n        # print(f\"train examples: {m}, correctly predicted: {correct}, accuracy: {correct/m}\")\n        return p, correct/m\n\n    @staticmethod\n    def compute_cost(AL, Y):\n        m = Y.shape[1]\n        cost = (-1./m) * np.sum(Y * np.log(AL))\n        cost = np.squeeze(cost)\n        assert cost.shape == ()\n        return cost\n    \n    def train_test_split(self, X, y, valid_split=0.2):\n        # Number of examples\n        num_examples = X.shape[1]\n        # Splitting the data\n        valid_split = int(num_examples * valid_split)\n        X_train = X[:, valid_split:]\n        y_train = y[:, valid_split:]\n        X_valid = X[:, :valid_split]\n        y_valid = y[:, :valid_split]\n\n        return X_train, y_train, X_valid, y_valid\n    \n    def plot_cost(self):\n        plt.figure(1)\n        plt.plot(np.squeeze(self.costs), linestyle=self.linestyle, color='C0')\n        plt.plot(np.squeeze(self.test_costs), linestyle=self.linestyle, color='C1')\n        # plt.legend(['train', 'test'], loc='upper right')\n        # plt.ylabel('cost')\n        # plt.xlabel('epoch')\n        # plt.title(\"Learning rate = \" + str(self.learning_rate))\n        # plt.savefig('figures/cost.png', dpi=300, bbox_inches='tight')\n        # plt.show()\n        \n    def plot_acc(self):\n        plt.figure(2)\n        plt.plot(np.squeeze(self.train_acc), linestyle=self.linestyle, color='C0')\n        plt.plot(np.squeeze(self.test_acc), linestyle=self.linestyle, color='C1')\n        # plt.legend(['train', 'test'], loc='upper left')\n        # plt.ylabel('accuracy')\n        # plt.xlabel('epoch')\n        # plt.title(\"Learning rate = \" + str(self.learning_rate))\n        # plt.savefig('figures/acc.png', dpi=300, bbox_inches='tight')\n        # plt.show()\n\n    def plot_f1(self):\n        plt.figure(3)\n        plt.plot(np.squeeze(self.test_f1_scores), linestyle=self.linestyle, color='C3')\n        # plt.ylabel('Validation Macro F1 score')\n        # plt.xlabel('epoch')\n        # plt.title(\"Learning rate = \" + str(self.learning_rate))\n        # plt.savefig('figures/f1.png', dpi=300, bbox_inches='tight')\n        # plt.show()\n\n    def save_model(self, filepath):\n        self.cache.clear()\n        self.costs.clear()\n        self.train_acc.clear()\n        self.test_acc.clear()\n        self.test_costs.clear()\n        self.test_f1_scores.clear()\n        for l in self.layers:\n            if isinstance(l, DenseLayer):\n                l.activation_function.activation_cache = None\n            elif isinstance(l, DropoutLayer):\n                l.mask = None\n        gc.collect()\n        with open(filepath, 'wb') as f:\n            pickle.dump(self, f)\n        \n    \ndef extract_data_and_labels(dataset):\n    # Initialize lists to store data and labels\n    data = []\n    labels = []\n\n    # Loop through the dataset\n    for image_tensor, label in tqdm(dataset):\n        # Flatten the image tensor and convert to numpy array\n        flattened_image = image_tensor.numpy().flatten()\n        data.append(flattened_image)\n        labels.append(label)\n\n    # Convert lists to numpy arrays\n    return np.array(data), np.array(labels)\n\n\ndef load_data(train=True):\n    print(\"Loading dataset for \" + (\"training\" if train else \"testing\") + \"...\")\n    dataset = ds.EMNIST(root='./data',\n                        split='letters',\n                        train=train,\n                        transform=transforms.ToTensor(),\n                        download=True)\n    X, y = extract_data_and_labels(dataset)\n    \n    X = X.T\n    y = y - 1\n    y_oh = np.eye(26)[y.astype('int32')]\n    y_oh = y_oh.T\n    return X, y_oh, y\n  \n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-01-01T16:55:01.509596Z","iopub.execute_input":"2024-01-01T16:55:01.509976Z","iopub.status.idle":"2024-01-01T16:55:06.221454Z","shell.execute_reply.started":"2024-01-01T16:55:01.509941Z","shell.execute_reply":"2024-01-01T16:55:06.220239Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"X_train, y_train, y_train_raw = load_data(train=True)\nX_test, y_test, y_test_raw = load_data(train=False)\nprint(X_train.shape)\n# Now shuffle the training set\nm_train = X_train.shape[1]\nnp.random.seed(1)\npermutation = list(np.random.permutation(m_train))\nX_train = X_train[:, permutation]\ny_train = y_train[:, permutation]\ny_train_raw = y_train_raw[permutation]","metadata":{"execution":{"iopub.status.busy":"2024-01-01T16:55:06.223345Z","iopub.execute_input":"2024-01-01T16:55:06.223936Z","iopub.status.idle":"2024-01-01T16:56:04.583061Z","shell.execute_reply.started":"2024-01-01T16:55:06.223900Z","shell.execute_reply":"2024-01-01T16:56:04.581711Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Loading dataset for training...\nDownloading https://www.itl.nist.gov/iaui/vip/cs_links/EMNIST/gzip.zip to ./data/EMNIST/raw/gzip.zip\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 561753746/561753746 [00:18<00:00, 30123954.59it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting ./data/EMNIST/raw/gzip.zip to ./data/EMNIST/raw\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 124800/124800 [00:17<00:00, 7332.40it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loading dataset for testing...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 20800/20800 [00:02<00:00, 7706.44it/s]\n","output_type":"stream"},{"name":"stdout","text":"(784, 124800)\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nsavefolder = 'fig_model1'\nos.mkdir(savefolder)\n","metadata":{"execution":{"iopub.status.busy":"2024-01-01T16:57:12.887446Z","iopub.execute_input":"2024-01-01T16:57:12.887937Z","iopub.status.idle":"2024-01-01T16:57:12.894029Z","shell.execute_reply.started":"2024-01-01T16:57:12.887896Z","shell.execute_reply":"2024-01-01T16:57:12.892664Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"!ls","metadata":{"execution":{"iopub.status.busy":"2024-01-01T16:57:25.385810Z","iopub.execute_input":"2024-01-01T16:57:25.386255Z","iopub.status.idle":"2024-01-01T16:57:26.505414Z","shell.execute_reply.started":"2024-01-01T16:57:25.386220Z","shell.execute_reply":"2024-01-01T16:57:26.503964Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"data  fig_model1\n","output_type":"stream"}]},{"cell_type":"code","source":"from matplotlib.lines import Line2D\nfrom matplotlib.patches import Patch\nplt.ioff()\nlearning_rates = [0.005, 0.001, 0.0005, 0.0001]\nfig_prefixes = ['005', '001', '0005', '0001']\nstyles = ['--', '-.', '-', ':']\nfor lr, prefix, s in zip(learning_rates, fig_prefixes, styles):\n    print(\"Using lr: \" + str(lr))\n    np.random.seed(1)\n    model = Network()\n    model.add(DenseLayer(input_size=X_train.shape[0], output_size=512))\n    model.add(DropoutLayer(rate=0.2))\n    model.add(DenseLayer(input_size=512, output_size=128))\n    model.add(DropoutLayer(rate=0.2))\n    model.add(DenseLayer(input_size=128, output_size=128))\n    model.add(DropoutLayer(rate=0.2))\n    model.add(DenseLayer(input_size=128, output_size=64))\n    model.add(DropoutLayer(rate=0.2))\n    model.add(DenseLayer(input_size=64, output_size=64))\n    model.add(DropoutLayer(rate=0.2))\n    model.add(DenseLayer(input_size=64, output_size=26, activation='softmax'))\n    # Assume X_train and y_train are defined\n    model.train(X_train, y_train, epochs=30, batch_size=1024, learning_rate=lr, valid_split=0.15, optimizer='adam', decay_rate=0.08, linestyle=s)\n    model.plot_cost()\n    model.plot_acc()\n    model.plot_f1()\n\n    y_hat, acc = model.predict(X_test, y_test)\n    print(f'Test accuracy: {acc}')\n    macro_f1 = f1_score(y_test_raw, y_hat, average='macro')\n    print(f'Test macro F1 score: {macro_f1:.5f}')\n\n    C_counts = confusion_matrix(y_test_raw, y_hat)\n    C = confusion_matrix(y_test_raw, y_hat, normalize='true')\n    plt.figure(4)\n    fig = plt.figure(figsize=(10, 10))\n    plt.imshow(C, 'Blues', vmax=0.1)\n    # also show the numbers\n    for i in range(26):\n        for j in range(26):\n            plt.text(j, i, f'{C_counts[i, j]}', horizontalalignment='center', verticalalignment='center', fontsize=8)\n    plt.xticks(range(26), labels=[chr(i+97) for i in range(26)])\n    plt.yticks(range(26), labels=[chr(i+97) for i in range(26)])\n    plt.xlabel('Predicted')\n    plt.ylabel('True')\n    plt.savefig(f'{savefolder}/{prefix}_conf.png', dpi=300, bbox_inches='tight')\n\nplt.figure(1)\nlegend_elements = [\n                  Line2D([0], [0], color='C0', lw=1.5, label='Train'),\n                  Line2D([0], [0], color='C1', lw=1.5, label='Validation'),\n                  Line2D([0], [0], color='black', lw=1.5, label='0.005', linestyle=styles[0]),\n                  Line2D([0], [0], color='black', lw=1.5, label='0.001', linestyle=styles[1]),\n                  Line2D([0], [0], color='black', lw=1.5, label='0.0005', linestyle=styles[2]),\n                  Line2D([0], [0], color='black', lw=1.5, label='0.0001', linestyle=styles[3])\n          ]\nplt.ylabel('cost')\nplt.xlabel('epoch')\nplt.title('Cost vs Epoch')\nplt.legend(handles=legend_elements, loc='upper right')\nplt.savefig(f'{savefolder}/cost.png', dpi=300, bbox_inches='tight')\n\nplt.figure(2)\nplt.xlabel('epoch')\nplt.ylabel('accuracy')\nplt.title('Accuracy vs Epoch')\nplt.legend(handles=legend_elements, loc='lower right')\nplt.savefig(f'{savefolder}/acc.png', dpi=300, bbox_inches='tight')\n\n\nlegend_elements = [\n                  Line2D([0], [0], color='black', lw=1.5, label='0.005', linestyle=styles[0]),\n                  Line2D([0], [0], color='black', lw=1.5, label='0.001', linestyle=styles[1]),\n                  Line2D([0], [0], color='black', lw=1.5, label='0.0005', linestyle=styles[2]),\n                  Line2D([0], [0], color='black', lw=1.5, label='0.0001', linestyle=styles[3])\n          ]\nplt.figure(3)\nplt.xlabel('epoch')\nplt.ylabel('macro-f1')\nplt.title('Validation Macro-F1 vs Epoch')\nplt.legend(handles=legend_elements, loc='lower right')\nplt.savefig(f'{savefolder}/f1.png', dpi=300, bbox_inches='tight')\n","metadata":{"execution":{"iopub.status.busy":"2024-01-01T16:58:16.382315Z","iopub.execute_input":"2024-01-01T16:58:16.382799Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Using lr: 0.005\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 104/104 [00:14<00:00,  7.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 1/30, train_cost: 3.88958, test_cost: 3.23936, train_acc: 0.07115, test_acc: 0.07025\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 104/104 [00:17<00:00,  6.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 2/30, train_cost: 3.57497, test_cost: 3.51329, train_acc: 0.12623, test_acc: 0.11993\n","output_type":"stream"},{"name":"stderr","text":" 70%|███████   | 73/104 [00:10<00:04,  7.31it/s]","output_type":"stream"}]}]}